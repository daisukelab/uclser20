{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import h5py\n",
    "\n",
    "data_hdf5 = h5py.File('work/FSDnoisy18k/data_hdf5/FSDnoisy18k_test.hdf5', mode='r')\n",
    "\n",
    "args = {'extract': {\n",
    "    'n_mels': 96,\n",
    "    'patch_hop': 50,\n",
    "    'patch_len': 101,\n",
    "}}\n",
    "\n",
    "def fetch_file_2_tensor(mel_spec, label_original, patch_hop, patch_len):\n",
    "    \"\"\"\n",
    "    Given a mel_spec, perform slicing into T-F patches, and store them in a list. Create a list of labels of same shape\n",
    "    inheriting clip-level labels.\n",
    "    :param mel_spec:\n",
    "    :return: two lists of patches and labels of same shape.\n",
    "    \"\"\"\n",
    "\n",
    "    idx = 0\n",
    "    start = 0\n",
    "    im_TF_patches = []\n",
    "    labels = []\n",
    "\n",
    "    while (start + patch_len) <= mel_spec.shape[0]:\n",
    "        im_TF_patches.append(mel_spec[start: start + patch_len])\n",
    "        labels.append(label_original)\n",
    "        # update indexes\n",
    "        start += patch_hop\n",
    "        idx += 1\n",
    "\n",
    "    return im_TF_patches, labels\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "for i in range(len(data_hdf5['binary_data'])):\n",
    "    im = np.ascontiguousarray(np.rec.array(np.frombuffer(data_hdf5['binary_data'][i])).reshape(-1, args['extract']['n_mels']))\n",
    "    im_TF_patches, labels_TF_patches = fetch_file_2_tensor(im, 0, args['extract']['patch_hop'], args['extract']['patch_len'])\n",
    "    data.append(np.asarray(im_TF_patches).astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.3956809560512295 0.00033731180361859254 0.0 48888192.0\n"
     ]
    }
   ],
   "source": [
    "labels_name = np.array(data_hdf5['labels'])\n",
    "\n",
    "# create dicts such that key: value is as follows\n",
    "# label: int\n",
    "# int: label\n",
    "list_labels = np.unique(labels_name)\n",
    "label_to_int = {k: v for v, k in enumerate(list_labels)}\n",
    "int_to_label = {v: k for k, v in label_to_int.items()}\n",
    "\n",
    "labels_original = []\n",
    "for k in labels_name:\n",
    "    labels_original.append(label_to_int[k])\n",
    "labels_original = np.asarray(labels_original)\n",
    "per_class_samples_accum = np.zeros((20,))\n",
    "per_class_samples_num = [[] for i in range(20)]\n",
    "per_class_samples_idx = [[] for i in range(20)]\n",
    "val_sum = 0.0\n",
    "num_values = 0.0\n",
    "## First loop to calculate data shape\n",
    "for i in range(len(data_hdf5['binary_data'])):\n",
    "\n",
    "    im = np.ascontiguousarray(np.rec.array(np.frombuffer(data_hdf5['binary_data'][i])).reshape(-1, args['extract']['n_mels']))\n",
    "    val_sum += im.sum()\n",
    "    num_values += im.size\n",
    "    file_frames = float(im.shape[0])  # number of time frames\n",
    "    # number of patches within clip\n",
    "    nb_inst = np.maximum(1, int(np.ceil((file_frames - args['extract']['patch_len']) / args['extract']['patch_hop'])))\n",
    "    per_class_samples_accum[labels_original[i]] += nb_inst\n",
    "    per_class_samples_num[labels_original[i]].append(nb_inst) #number of TF patches for each class and spectogram\n",
    "    per_class_samples_idx[labels_original[i]].append(i) #overall indexes per-class\n",
    "\n",
    "spec_mean = val_sum / num_values\n",
    "val_std = 0.0\n",
    "\n",
    "\n",
    "train_size = int(per_class_samples_accum.sum())\n",
    "# initialize data and labels to be returned\n",
    "data = np.zeros((train_size, args['extract']['patch_len'], args['extract']['n_mels'])).astype('float32')\n",
    "labels = np.zeros((train_size,)).astype(int)\n",
    "train_count = 0\n",
    "for i in range(len(data_hdf5['binary_data'])):\n",
    "    im = np.ascontiguousarray(np.rec.array(np.frombuffer(data_hdf5['binary_data'][i])).reshape(-1, args['extract']['n_mels']))\n",
    "    val_std += ((im - spec_mean) ** 2).sum()\n",
    "\n",
    "spec_std = np.sqrt(val_std) / num_values\n",
    "\n",
    "print(spec_mean, spec_std, np.mean(np.vstack(data)), num_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Possible issue: std might be too small because dividing too big num_values.\n",
    "- The mean/std calculated from the training set was: [-1.4, 6e-5].\n",
    "    - Note that the std is much smaller than test set's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(-1.3956809560512295, 0.00033731180361859254),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-efe05b9c37ce>:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  args = yaml.load(open('config/params_supervised_lineval.yaml'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 4142.4980,  3218.7227,  -731.8396,  ...,  4195.0850,\n",
       "            4912.4370,  3942.5151],\n",
       "          [ 6402.0044,  3796.8489,  -375.2035,  ...,  5460.7759,\n",
       "            5689.5244,  4117.4517],\n",
       "          [ 5786.2642,  3043.6792,  -145.8010,  ...,  5909.9009,\n",
       "            6809.0645,  5598.4883],\n",
       "          ...,\n",
       "          [ 5782.3354,  5604.1104,  5531.8413,  ...,  9478.7383,\n",
       "            9411.0635,  8599.8369],\n",
       "          [ 9423.4443,  7158.6445,  4928.6548,  ..., 11146.4424,\n",
       "           11065.3125,  9907.1582],\n",
       "          [ 9782.8154,  5573.3979,  5427.0586,  ...,  9104.5752,\n",
       "            8962.4521,  8343.3369]]]),\n",
       " array([9]),\n",
       " 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import FSDnoisy18k.dataset.FSDnoisy18k\n",
    "import yaml\n",
    "args = yaml.load(open('config/params_supervised_lineval.yaml'))\n",
    "testset = FSDnoisy18k.dataset.FSDnoisy18k.FSDnoisy18k(args, mode='test', hdf5_path='work/FSDnoisy18k/data_hdf5/')\n",
    "testset.transform = test_transform\n",
    "testset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(580.5177), tensor(6605.0996), tensor(11756.6436), tensor(-14670.8662))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = testset[0][0]\n",
    "x.mean(), x.std(), x.max(), x.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
